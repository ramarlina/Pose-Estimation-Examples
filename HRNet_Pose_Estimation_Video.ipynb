{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HRNet - Pose Estimation - Video.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6Zn_B-TmJbF",
        "colab_type": "text"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM6ZbFC5ll0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# installing Facebook's detectron 2 for person detector\n",
        "!pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html \n",
        "!pip install cython pyyaml==5.1\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html\n",
        "\n",
        "!git clone https://github.com/facebookresearch/detectron2.git\n",
        "\n",
        "# This is my fork of HRNet, this is the same as the official repo, minus some dependencies\n",
        "# The official repo will work just as well, you'll just have to make sure to install requirements  \n",
        "!git clone https://github.com/ramarlina/Higher-HRNet-Human-Pose-Estimation.git\n",
        "\n",
        "# downloading pretrained weights from the official Google Drive repository\n",
        "!gdown https://drive.google.com/uc?id=1V9Iz0ZYy9m8VeaspfKECDW0NKlGsYmO1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbRttZZZIDyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding repo to python's paths since we're not going to install it\n",
        "import sys \n",
        "sys.path.append(\"Higher-HRNet-Human-Pose-Estimation/lib\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uly6Eg7ponjt",
        "colab_type": "text"
      },
      "source": [
        "# Creating a HRNet Pose Estimation model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zRrhW27mI7k",
        "colab_type": "text"
      },
      "source": [
        "Some custom code for parsing the yaml config file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xISKxHxkmPTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json \n",
        "import yaml\n",
        "\n",
        "# Loading the yaml file\n",
        "config_file = \"Higher-HRNet-Human-Pose-Estimation/experiments/coco/higher_hrnet/w32_512_adam_lr1e-3.yaml\"\n",
        "config_json = yaml.load(open(config_file))\n",
        " \n",
        "def walk(node):\n",
        "    obj = {}\n",
        "    for key, item in node.items():\n",
        "        if isinstance(item, dict): \n",
        "            obj[key] = ConfigParser(item)\n",
        "        else:\n",
        "            obj[key] = item\n",
        "    return obj\n",
        "\n",
        "# Custom parser class \n",
        "class ConfigParser():\n",
        "    def __init__(self, cfg_json): \n",
        "        self.__dict__ = walk(cfg_json) \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.__dict__[idx]\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        self.__dict__[key] = value\n",
        "\n",
        "    def __repr__(self):\n",
        "        return json.dumps(list(self.__dict__.keys()))\n",
        "\n",
        "config = ConfigParser(config_json)\n",
        " \n",
        "print(\"Num Joints: \", config.MODEL.NUM_JOINTS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HyMInt1mT5M",
        "colab_type": "text"
      },
      "source": [
        "Instantiating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2LbGtPlmRDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from models.pose_higher_hrnet import PoseHigherResolutionNet \n",
        "import torch\n",
        "\n",
        "# set this to \"cuda\" to use GPU\n",
        "device = \"cpu\" \n",
        "\n",
        "# creating the model\n",
        "model = PoseHigherResolutionNet(config).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCavyPuRoT8p",
        "colab_type": "text"
      },
      "source": [
        "Loading pre-trained weights from the official Google Drive repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUA5sDuFoZth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading weights\n",
        "state_dict = torch.load(\"./pose_higher_hrnet_w32_512.pth\")\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZzwDYICohF5",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GgctOllrEXX",
        "colab_type": "text"
      },
      "source": [
        "Helper functions for loading and preprocessing of an image and for predicting pose using the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnV0hv3ER4Nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils.transforms import resize_align_multi_scale \n",
        "from utils.transforms import get_multi_scale_size\n",
        "import cv2\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt \n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "transforms = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "])\n",
        " \n",
        "def preproc_image(image, resolution=(512,512)): \n",
        "    if isinstance(image, str):\n",
        "        image = cv2.imread(image)   \n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
        "\n",
        "    image_resized = cv2.resize(image, resolution)\n",
        "\n",
        "    image_resized = transforms(image_resized)\n",
        "\n",
        "    image_resized = image_resized.unsqueeze(0)\n",
        "    return image, image_resized\n",
        "\n",
        "def predict(model, X, original_size): \n",
        "    model.eval()\n",
        "    outputs = model(X)\n",
        "\n",
        "    n_joints = outputs[-1].shape[1]\n",
        "\n",
        "    hm = 0\n",
        "    for i, output in enumerate(outputs):\n",
        "        if i < len(outputs):\n",
        "            output = torch.nn.functional.interpolate(\n",
        "                output,\n",
        "                size=(original_size[0], original_size[1]),\n",
        "                mode='bilinear',\n",
        "                align_corners=False\n",
        "            )\n",
        "        hm += output[:, :n_joints].detach().cpu().numpy()\n",
        "\n",
        "    hm /= 2\n",
        "    \n",
        "    hm = gaussian_filter(hm, sigma=.5)\n",
        "\n",
        "    pts = np.zeros((n_joints, 3)) \n",
        "\n",
        "    for i, joint in enumerate(hm[0]):  \n",
        "        pt = np.unravel_index(np.argmax(joint), joint.shape)\n",
        "        pts[i:, :2] = pt[::-1]   \n",
        "        pts[i:, 2] = joint[pt] \n",
        "        \n",
        "    return pts\n",
        "\n",
        "def visualize_pose(image, pts):\n",
        "    \"\"\"\n",
        "        Visualizing predicted poses\n",
        "    \"\"\"\n",
        "    skeleton = [ \n",
        "        [15, 13], [13, 11], [16, 14], [14, 12], [11, 12], [5, 11], [6, 12], [5, 6], [5, 7],\n",
        "        [6, 8], [7, 9], [8, 10], [1, 2], [0, 1], [0, 2], [1, 3], [2, 4],  # [3, 5], [4, 6]\n",
        "        [0, 5], [0, 6]\n",
        "    ] \n",
        "\n",
        "    for i, joint in enumerate(skeleton):\n",
        "        pt1, pt2 = pts[joint] \n",
        "        if pt1[2] > 0.1 and pt2[2] > 0.1:\n",
        "            image = cv2.line(\n",
        "                image, (int(pt1[0]), int(pt1[1])), (int(pt2[0]), int(pt2[1])),\n",
        "                (0,255,0), 5\n",
        "            )\n",
        "\n",
        "    for pt in pts:\n",
        "        if pt[2] > 0.1:\n",
        "            image = cv2.circle(image, (int(pt[0]), int(pt[1])), 10, (255,0,0), -1)\n",
        "\n",
        "    return image \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo-8MI2DL2oc",
        "colab_type": "text"
      },
      "source": [
        "# Predicting body pose in video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdsVWV2nnd0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import some common detectron2 utilities\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg \n",
        "\n",
        "# Create config\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"./detectron2/configs/COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 \n",
        "cfg.MODEL.WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_101_FPN_3x/137851257/model_final_f6e8b1.pkl\"\n",
        "\n",
        "# Create detector\n",
        "detector = DefaultPredictor(cfg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpVbJW8XPUJQ",
        "colab_type": "text"
      },
      "source": [
        "Video by Wolfgang Sauerwald from Pexels:\n",
        "\n",
        "https://www.pexels.com/video/dancing-on-the-street-3608987/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LIxnytQL7ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://static.haizaha.com/dancing-on-the-street-3608987.mp4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvecovJnYbkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "cap = cv2.VideoCapture('dancing-on-the-street-3608987.mp4')\n",
        "\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')  \n",
        "out = cv2.VideoWriter('output.avi', fourcc, fps, (1920,1080))\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "pbar = tqdm(total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "\n",
        "while(cap.isOpened()):\n",
        "    pbar.update(1)\n",
        "\n",
        "    ret, frame = cap.read()\n",
        "    \n",
        "    if(ret == False):\n",
        "        break\n",
        "\n",
        "    pose = np.zeros(frame.shape)\n",
        "    viz = np.array(frame)\n",
        "     \n",
        "\n",
        "    # detecting people within frame\n",
        "    pred_detection = detector(frame)\n",
        "    boxes   = pred_detection[\"instances\"].pred_boxes\n",
        "    classes = pred_detection[\"instances\"].pred_classes\n",
        "    scores  = pred_detection[\"instances\"].scores\n",
        "    \n",
        "    for box, cid, score in zip(boxes, classes, scores): \n",
        "        if score > 0.9 and cid == 0:\n",
        "            # centering person box\n",
        "            box = box.detach().cpu().numpy().astype(\"i\")[[1,0,3,2]] \n",
        "            h, w, c = box[2]-box[0], box[3]-box[1], (box[[0,2]].sum() // 2, box[[1,3]].sum() // 2)\n",
        "            r = max(w, h) // 2 + 100\n",
        "            box = np.array([c[0] - r, c[1] - r, c[0] + r, c[1] + r])\n",
        "            box[[0,2]] = np.clip(box[[0,2]], 0, frame.shape[0])\n",
        "            box[[1,3]] = np.clip(box[[1,3]], 0, frame.shape[1])   \n",
        "\n",
        "            # cropping, transforming, and tensorifying image\n",
        "            image, X = preproc_image(frame[box[0]:box[2], box[1]:box[3]]) \n",
        "\n",
        "            # detecting pose\n",
        "            pts = predict(model, X.cuda(), image.shape[:-1])\n",
        "            \n",
        "            # offsetting point coordinates with respect to box\n",
        "            pts[:,:2] += box[[1,0]]\n",
        "\n",
        "            # drawing points and skeleton\n",
        "            viz = visualize_pose(viz, pts)\n",
        "            pose = visualize_pose(pose, pts)\n",
        "            viz[:360, :640] = cv2.resize(pose, (640,360))\n",
        "   \n",
        "    out.write(viz)\n",
        "    \n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGQrJn2pboCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting the video to mp4\n",
        "!ffmpeg -i output.avi output_mm.mp4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrU6VeK6QaYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}